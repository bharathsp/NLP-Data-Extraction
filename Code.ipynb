{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8401802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DELETED] Folder 'output_articles'\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241017.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241018.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241019.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241020.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241021.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241022.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241023.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241024.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241025.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241026.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241027.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241028.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241029.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241030.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241031.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241032.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241033.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241034.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241035.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241036.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241037.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241038.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241039.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241040.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241041.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241042.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241043.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241044.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241045.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241046.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241047.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241048.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241049.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241050.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241051.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241052.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241053.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241054.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241055.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241056.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241057.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241058.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241059.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241060.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241061.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241062.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241063.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241064.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241065.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241066.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241067.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241068.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241069.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241070.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241071.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241072.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241073.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241074.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241075.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241076.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241077.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241078.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241079.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241080.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241081.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241082.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241083.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241084.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241085.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241086.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241087.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241088.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241089.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241090.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241091.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241092.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241093.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241094.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241095.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241096.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241097.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241098.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241099.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241100.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241101.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241102.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241103.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241104.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241105.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241106.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241107.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241108.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241109.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241110.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241111.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241112.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241113.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241114.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241115.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241116.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241117.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241118.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241119.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241120.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241121.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241122.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241123.txt\n",
      "[ERROR] Failed to fetch URL: https://insights.blackcoffer.com/microsoft-azure-chatbot-with-luis-language-understanding/ | Status Code: 404\n",
      "[FAILED] Skipped: Netclan20241124\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241125.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241126.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241127.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241128.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241129.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241130.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241131.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241132.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241133.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241134.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241135.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241136.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241137.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241138.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241139.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241140.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241141.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241142.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241143.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241144.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241145.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241146.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241147.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241148.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241149.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241150.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241151.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241152.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241153.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241154.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241155.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241156.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241157.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241158.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241159.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241160.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241161.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241162.txt\n",
      "[SUCCESS] Saved: output_articles\\Netclan20241163.txt\n",
      "[Missing] output_articles\\Netclan20241124.txt\n",
      "[✓] Text analysis complete. Output saved to Text_Analysis_Output.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Excel file\n",
    "input_df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "folder_path = 'output_articles'\n",
    "\n",
    "# Delete the folder if it exists\n",
    "if os.path.exists(folder_path):\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"[DELETED] Folder '{folder_path}'\")\n",
    "\n",
    "# Create output folder if not exists\n",
    "os.makedirs(folder_path)\n",
    "\n",
    "# Function to extract title and article text from a given URL\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                          \"Chrome/124.0.0.0 Safari/537.36\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"[ERROR] Failed to fetch URL: {url} | Status Code: {response.status_code}\")\n",
    "            return None, None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Flexible title extraction\n",
    "        if soup.find('h1', class_='tdb-title-text'):\n",
    "            title = soup.find('h1', class_='tdb-title-text').get_text(strip=True)\n",
    "        elif soup.find('h1'):\n",
    "            title = soup.find('h1').get_text(strip=True)\n",
    "        elif soup.title:\n",
    "            title = soup.title.get_text(strip=True)\n",
    "        else:\n",
    "            strong = soup.find(['strong', 'b'])\n",
    "            title = strong.get_text(strip=True) if strong else \"No Title Found\"\n",
    "\n",
    "        # Article text\n",
    "        content_div = soup.find('div', class_='td-post-content')\n",
    "        paragraphs = content_div.find_all('p') if content_div else []\n",
    "        article_text = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        return title, article_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[EXCEPTION] {url} - {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load stop words from all files in StopWords/\n",
    "def load_stopwords(folder):\n",
    "    stopwords = set()\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder, file)\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    stopwords.add(line.strip().lower())\n",
    "        except UnicodeDecodeError:\n",
    "            # Retry with latin-1 encoding\n",
    "            with open(path, 'r', encoding='latin-1') as f:\n",
    "                for line in f:\n",
    "                    stopwords.add(line.strip().lower())\n",
    "    return stopwords\n",
    "\n",
    "stop_words = load_stopwords('StopWords')\n",
    "\n",
    "\n",
    "# Load master dictionary\n",
    "def load_words(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            return set(line.strip().lower() for line in file)\n",
    "    except UnicodeDecodeError:\n",
    "        # Try with fallback encoding (commonly used: latin-1)\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            return set(line.strip().lower() for line in file)\n",
    "\n",
    "\n",
    "positive_words = load_words('MasterDictionary/positive-words.txt') - stop_words\n",
    "negative_words = load_words('MasterDictionary/negative-words.txt') - stop_words\n",
    "\n",
    "# Count syllables\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiou\"\n",
    "    count = 0\n",
    "    if word.endswith((\"es\", \"ed\")):\n",
    "        word = word[:-2]\n",
    "    for idx, char in enumerate(word):\n",
    "        if char in vowels and (idx == 0 or word[idx-1] not in vowels):\n",
    "            count += 1\n",
    "    return max(count, 1)\n",
    "\n",
    "# Count complex words\n",
    "def count_complex_words(words):\n",
    "    return sum(1 for word in words if count_syllables(word) > 2)\n",
    "\n",
    "# Clean and tokenize text\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    clean_words = [w for w in tokens if w not in stop_words]\n",
    "    return clean_words\n",
    "\n",
    "# Personal pronoun counter\n",
    "def count_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
    "    return len([p for p in pronouns if p.lower() != 'us'])\n",
    "\n",
    "# Analyze text\n",
    "def analyze_text(article_text):\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    words = clean_text(article_text)\n",
    "\n",
    "    pos_score = sum(1 for word in words if word in positive_words)\n",
    "    neg_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity = (pos_score - neg_score) / ((pos_score + neg_score) + 1e-6)\n",
    "    subjectivity = (pos_score + neg_score) / (len(words) + 1e-6)\n",
    "\n",
    "    avg_sent_len = len(words) / len(sentences) if sentences else 0\n",
    "    complex_words = count_complex_words(words)\n",
    "    percent_complex = complex_words / len(words) if words else 0\n",
    "    fog_index = 0.4 * (avg_sent_len + percent_complex)\n",
    "    avg_words_per_sent = avg_sent_len\n",
    "    syllable_per_word = sum(count_syllables(word) for word in words) / len(words) if words else 0\n",
    "    personal_pronouns = count_personal_pronouns(article_text)\n",
    "    avg_word_len = sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "    return [\n",
    "        pos_score, neg_score, polarity, subjectivity,\n",
    "        avg_sent_len, percent_complex, fog_index, avg_words_per_sent,\n",
    "        complex_words, len(words), syllable_per_word, personal_pronouns,\n",
    "        avg_word_len\n",
    "    ]\n",
    "\n",
    "\n",
    "# Loop through each row and extract text\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    title, article = extract_article_text(url)\n",
    "\n",
    "    if title and article:\n",
    "        output_path = os.path.join('output_articles', f\"{url_id}.txt\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(title + \"\\n\\n\" + article)\n",
    "        print(f\"[SUCCESS] Saved: {output_path}\")\n",
    "    else:\n",
    "        print(f\"[FAILED] Skipped: {url_id}\")\n",
    "\n",
    "# Final output\n",
    "output = []\n",
    "\n",
    "for _, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    file_path = os.path.join('output_articles', f\"{url_id}.txt\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "        article_data = analyze_text(full_text)\n",
    "        output.append([url_id, url] + article_data)\n",
    "    else:\n",
    "        print(f\"[Missing] {file_path}\")\n",
    "        output.append([url_id, url] + [None]*13)\n",
    "\n",
    "# Output to Excel\n",
    "columns = [\n",
    "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
    "    'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "]\n",
    "\n",
    "output_df = pd.DataFrame(output, columns=columns)\n",
    "output_df.to_excel(\"Text_Analysis_Output.xlsx\", index=False)\n",
    "print(\"[✓] Text analysis complete. Output saved to Text_Analysis_Output.xlsx.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a089da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
